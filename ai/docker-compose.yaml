# =================================================================
# ðŸš€ ÐŸÐžÐ’ÐÐ˜Ð™ ÐÐ›Ð“ÐžÐ Ð˜Ð¢Ðœ Ð”Ð†Ð™: Ð’Ð†Ð” Ð—ÐÐŸÐ£Ð¡ÐšÐ£ Ð”Ðž Ð ÐžÐ‘ÐžÐ¢Ð˜ Ð’ IDE
# =================================================================
# 
# ðŸ›  ÐšÐ ÐžÐš 1: Ð—ÐÐŸÐ£Ð¡Ðš Ð£ Ð¢Ð•Ð ÐœÐ†ÐÐÐ›Ð†
#    docker compose up -d
#
# ðŸ“¥ ÐšÐ ÐžÐš 2: ÐœÐžÐÐ†Ð¢ÐžÐ Ð˜ÐÐ“ Ð¢Ð ÐŸÐ•Ð Ð•Ð’Ð†Ð ÐšÐ
#    docker logs -f ollama-init
#    docker exec -it ollama ollama list
#    docker exec -it ollama nvidia-smi
#
# ðŸŒ ÐšÐ ÐžÐš 3: Ð’Ð¥Ð†Ð” Ð’ OPEN WEBUI
#    http://localhost:3000
#
# ðŸ’» ÐšÐ ÐžÐš 4: ÐŸÐ†Ð”ÐšÐ›Ð®Ð§Ð•ÐÐÐ¯ Ð”Ðž INTELLIJ IDEA
#    Settings -> Tools -> AI Assistant -> Enable Local Models
#    Address: http://localhost:11434
#    Chat: qwen2.5:14b | Code Completion: qwen2.5-coder:7b
# =================================================================

services:
  # --- Ð”Ð²Ð¸Ð³ÑƒÐ½ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (GPU) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- ÐÐ²Ñ‚Ð¾-Ñ–Ð½ÑÑ‚Ð°Ð»ÑÑ‚Ð¾Ñ€ Ð· Ð²ÑˆÐ¸Ñ‚Ð¸Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ 16k ---
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    volumes:
      - ollama_models:/root/.ollama
    depends_on:
      - ollama
    entrypoint: ["/usr/bin/sh", "-c"]
    command:
      - |
        sleep 5
        ollama pull qwen2.5:14b
        ollama pull qwen2.5-coder:7b
        ollama pull nomic-embed-text
        echo 'FROM qwen2.5:14b\nPARAMETER num_ctx 8192' > /tmp/Modelfile
        ollama create qwen2.5:14b -f /tmp/Modelfile
    restart: "no"

  # --- Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð° Ð±Ð°Ð·Ð° Ð´Ð°Ð½Ð¸Ñ… ---
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    shm_size: "1gb"

  # --- Ð†Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð° (RAG Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€) ---
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: always
    ports:
      - "3000:8080"  # Ð¼Ð°Ð¿Ð¸Ð¼Ð¾ 3000 Ð½Ð° Ñ…Ð¾ÑÑ‚Ñ– -> 8080 Ñƒ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ–
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - VECTOR_DB_TYPE=qdrant
      - VECTOR_DB_URL=http://qdrant:6333
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - RAG_TOP_K=10
      - RAG_RELEVANCE_THRESHOLD=0.4
      - ENABLE_RAG_WEB_SEARCH=True
      - WEBUI_SECRET_KEY=ost_secret_key_2025_strong
      - WEBUI_AUTH=True
      - DEFAULT_MODEL=qwen2.5:14b
    volumes:
      - openwebui_data:/app/data
    depends_on:
      - ollama
      - qdrant
    user: root
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  ollama_models:
  openwebui_data:
  qdrant_data:
