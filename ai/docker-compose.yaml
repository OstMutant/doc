version: "3.9"

# =================================================================
# üöÄ –ü–û–í–ù–ò–ô –ê–õ–ì–û–†–ò–¢–ú –î–Ü–ô: –í–Ü–î –ó–ê–ü–£–°–ö–£ –î–û –†–û–ë–û–¢–ò –í IDE
# =================================================================
# 
# üõ† –ö–†–û–ö 1: –ó–ê–ü–£–°–ö –£ –¢–ï–†–ú–Ü–ù–ê–õ–Ü
#    1. –í—ñ–¥–∫—Ä–∏–π—Ç–µ PowerShell –∞–±–æ WSL —É –ø–∞–ø—Ü—ñ –∑ —Ü–∏–º —Ñ–∞–π–ª–æ–º.
#    2. –í–≤–µ–¥—ñ—Ç—å: docker compose up -d
#
# üì• –ö–†–û–ö 2: –ú–û–ù–Ü–¢–û–†–ò–ù–ì –¢–ê –ü–ï–†–ï–í–Ü–†–ö–ê
#    1. –í–≤–µ–¥—ñ—Ç—å: docker logs -f ollama-init
#    2. üß™ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π: docker exec -it ollama ollama list
#       (–ú–∞—î –±—É—Ç–∏: qwen2.5:14b, qwen2.5-coder:7b, nomic-embed-text)
#
# üåê –ö–†–û–ö 3: –í–•–Ü–î –í OPEN WEBUI (–î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ .md)
#    1. –í—ñ–¥–∫—Ä–∏–π—Ç–µ: http://localhost:3000
#    2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ –¥–æ–∫–∏ –≤ "Workspace" -> "Documents".
#
# üíª –ö–†–û–ö 4: –ü–Ü–î–ö–õ–Æ–ß–ï–ù–ù–Ø –î–û INTELLIJ IDEA (–†—ñ–¥–Ω–∏–π AI –ü–ª–∞–≥—ñ–Ω)
#    1. Settings -> Tools -> AI Assistant.
#    2. –£–≤—ñ–º–∫–Ω—ñ—Ç—å "Enable Local Models" —Ç–∞ –æ–±–µ—Ä—ñ—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä "Ollama".
#    3. –í–∫–∞–∂—ñ—Ç—å –∞–¥—Ä–µ—Å—É: http://localhost:11434
#    4. Chat: qwen2.5:14b | Code Completion: qwen2.5-coder:7b
#    5. –ü–†–ò–ú–Ü–¢–ö–ê: –ö–æ–Ω—Ç–µ–∫—Å—Ç 16384 –≤–∂–µ –≤—à–∏—Ç–∏–π —É –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ!
# =================================================================

services:
  # --- –î–≤–∏–≥—É–Ω –º–æ–¥–µ–ª–µ–π (GPU) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- –ê–≤—Ç–æ-—ñ–Ω—Å—Ç–∞–ª—è—Ç–æ—Ä –∑ –≤—à–∏—Ç–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 16k ---
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    environment:
      - OLLAMA_HOST=ollama:11434
    volumes:
      - ollama_models:/root/.ollama
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "sleep 5 && 
             ollama pull qwen2.5:14b && 
             ollama pull qwen2.5-coder:7b && 
             ollama pull nomic-embed-text && 
             echo 'FROM qwen2.5:14b\nPARAMETER num_ctx 16384' > /tmp/Modelfile && 
             ollama create qwen2.5:14b -f /tmp/Modelfile"
    restart: on-failure

  # --- –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö ---
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    shm_size: "1gb"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (RAG –º–µ–Ω–µ–¥–∂–µ—Ä) ---
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: always
    ports:
      - "3000:3000"
    environment:
      - WEBUI_PORT=3000
      - OLLAMA_BASE_URL=http://ollama:11434
      - VECTOR_DB_TYPE=qdrant
      - VECTOR_DB_URL=http://qdrant:6333
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - RAG_TOP_K=10
      - RAG_RELEVANCE_THRESHOLD=0.4
      - ENABLE_RAG_WEB_SEARCH=True
      - WEBUI_SECRET_KEY=ost_secret_key_2025_strong
      - WEBUI_AUTH=True
      - DEFAULT_MODEL=qwen2.5:14b
    volumes:
      - openwebui_data:/app/data
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    user: root
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  ollama_models:
  openwebui_data:
  qdrant_data: