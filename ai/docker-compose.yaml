version: "3.9"

# =================================================================
# üöÄ –ü–û–í–ù–ò–ô –ê–õ–ì–û–†–ò–¢–ú –î–Ü–ô: –í–Ü–î –ó–ê–ü–£–°–ö–£ –î–û –†–û–ë–û–¢–ò –í IDE
# =================================================================
# 
# üõ† –ö–†–û–ö 1: –ó–ê–ü–£–°–ö –£ –¢–ï–†–ú–Ü–ù–ê–õ–Ü
#    1. –í—ñ–¥–∫—Ä–∏–π—Ç–µ PowerShell –∞–±–æ WSL —É –ø–∞–ø—Ü—ñ –∑ —Ü–∏–º —Ñ–∞–π–ª–æ–º.
#    2. –í–≤–µ–¥—ñ—Ç—å: docker compose up -d
#
# üì• –ö–†–û–ö 2: –ú–û–ù–Ü–¢–û–†–ò–ù–ì –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø (–í–∞–∂–ª–∏–≤–æ!)
#    1. –í–≤–µ–¥—ñ—Ç—å: docker logs -f ollama-init
#    2. –î–æ—á–µ–∫–∞–π—Ç–µ—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è (—É—Å–ø—ñ—à–Ω–∏–π pull –≤—Å—ñ—Ö 3 –º–æ–¥–µ–ª–µ–π).
#
# üåê –ö–†–û–ö 3: –í–•–Ü–î –í OPEN WEBUI (–î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ .md)
#    1. –í—ñ–¥–∫—Ä–∏–π—Ç–µ: http://localhost:3000
#    2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ –¥–æ–∫–∏ –≤ "Workspace" -> "Documents".
#
# üíª –ö–†–û–ö 4: –ü–Ü–î–ö–õ–Æ–ß–ï–ù–ù–Ø –î–û INTELLIJ IDEA (–†—ñ–¥–Ω–∏–π AI –ü–ª–∞–≥—ñ–Ω)
#    1. Settings -> Tools -> AI Assistant.
#    2. –£–≤—ñ–º–∫–Ω—ñ—Ç—å "Enable Local Models" —Ç–∞ –æ–±–µ—Ä—ñ—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä "Ollama".
#    3. –í–∫–∞–∂—ñ—Ç—å –∞–¥—Ä–µ—Å—É: http://localhost:11434
#    4. –£ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö –º–æ–¥–µ–ª—ñ –≤ IDEA –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –≤–∏–±—Ä–∞–Ω–æ:
#       - Chat: qwen2.5:14b
#       - Code Completion: qwen2.5-coder:7b
#    5. –ü–†–ò–ú–Ü–¢–ö–ê: –ö–æ–Ω—Ç–µ–∫—Å—Ç 16384 –≤–∂–µ –≤—à–∏—Ç–∏–π —É –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ!
# =================================================================

services:
  # --- –î–≤–∏–≥—É–Ω –º–æ–¥–µ–ª–µ–π (GPU) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    environment:
      - OLLAMA_NUM_PARALLEL=4      # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ —á–∞—Ç—É —Ç–∞ –∞–≤—Ç–æ–¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è
      - OLLAMA_MAX_LOADED_MODELS=2 # 14b —Ç–∞ 7b —Ç—Ä–∏–º–∞—é—Ç—å—Å—è —É VRAM –æ–¥–Ω–æ—á–∞—Å–Ω–æ
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- –ê–≤—Ç–æ-—ñ–Ω—Å—Ç–∞–ª—è—Ç–æ—Ä –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É 16k ---
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    environment:
      - OLLAMA_HOST=ollama:11434
    volumes:
      - ollama_models:/root/.ollama
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "sleep 5 && 
             ollama pull qwen2.5:14b && 
             ollama pull qwen2.5-coder:7b && 
             ollama pull nomic-embed-text && 
             echo 'FROM qwen2.5:14b\nPARAMETER num_ctx 16384' > /tmp/Modelfile && 
             ollama create qwen2.5:14b -f /tmp/Modelfile"
    restart: on-failure

  # --- –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö ---
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    shm_size: "1gb"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (RAG –º–µ–Ω–µ–¥–∂–µ—Ä) ---
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: always
    ports:
      - "3000:3000"  # –¢–µ–ø–µ—Ä –ø–æ—Ä—Ç–∏ —Å–ø—ñ–≤–ø–∞–¥–∞—é—Ç—å —è–≤–Ω–æ
    environment:
      - WEBUI_PORT=3000
      - OLLAMA_BASE_URL=http://ollama:11434
      - VECTOR_DB_TYPE=qdrant
      - VECTOR_DB_URL=http://qdrant:6333
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - RAG_TOP_K=10
      - RAG_RELEVANCE_THRESHOLD=0.4
      - ENABLE_RAG_WEB_SEARCH=True
      - WEBUI_SECRET_KEY=ost_secret_key_2025_strong
      - WEBUI_AUTH=True
      - DEFAULT_MODEL=qwen2.5:14b
    volumes:
      - openwebui_data:/app/data
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    user: root
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  ollama_models:
  openwebui_data:
  qdrant_data: