-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Guava Cache (https://www.baeldung.com/guava-cache) 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	Docs:
		https://dzone.com/articles/google-guava-cache
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Redis (https://en.wikipedia.org/wiki/Redis) 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	Redis (/ˈrɛdɪs/;[6][7] Remote Dictionary Server)[6] is an open-source in-memory storage, used as a distributed, in-memory key–value database, cache and message broker, with optional durability.
		Because it holds all data in memory and because of its design, Redis offers low-latency reads and writes, making it particularly suitable for use cases that require a cache. 
	Docs:
		https://habr.com/ru/companies/wunderfund/articles/685894/
	Implementation
		https://www.baeldung.com/spring-data-redis-tutorial - Introduction to Spring Data Redis
		https://javatute.com/java-lang-noclassdeffounderror-org-apache-commons-pool2-impl-genericobjectpoolconfig/ - maven update
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
What is: Multifactor Authentication(https://support.microsoft.com/en-us/topic/what-is-multifactor-authentication-e5e39437-121c-be60-d123-eda06bddf661)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	A factor in authentication is a way of confirming your identity when you try to sign in. For example, a password is one kind of factor, it's a thing you know. The three most common kinds of factors are: 
		Something you know - Like a password, or a memorized PIN. 
		Something you have - Like a smartphone, or a secure USB key. 
		Something you are - Like a fingerprint, or facial recognition. 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Apache Kafka (https://habr.com/ru/company/piter/blog/352978/)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	Kafka - распределенный горизонтально масштабируемый отказоустойчивый журнал коммитов.
		Распределенный
			Распределенной называется такая система, которая в сегментированном виде работает сразу на множестве машин,
				образующих цельный кластер; поэтому для конечного пользователя они выглядят как единый узел. 
				Распределенность Kafka заключается в том, что хранение, получение и рассылка сообщений у него организовано на разных узлах (так называемых «брокерах»).
			Важнейшие плюсы такого подхода – высокодоступность и отказоустойчивость.
		Вертикальное масштабирование – на машину навешиваются дополнительные ресурсы. При таком «масштабировании вверх» возникает два серьезных недостатка:
			Есть определенные пределы, связанные с возможностями оборудования. Бесконечно наращиваться нельзя.
			Такая работа обычно связана с простоями, а большие компании не могут позволить себе простоев.
		Горизонтальная масштабируемость решает ровно ту же проблему, просто мы подключаем к делу все больше машин. 
			При добавлении новой машины никаких простоев не происходит, при этом, количество машин, которые можно добавить в кластер, ничем не ограничено. 
			
	What is Apache Kafka (https://cloud.google.com/learn/what-is-apache-kafka)
		Apache Kafka is a popular event streaming platform used to collect, process, and store streaming event data or data that has no discrete beginning or end.
		Kafka makes possible a new generation of distributed applications capable of scaling to handle billions of streamed events per minute.
		
	В архитектуре Kafka Apache ключевыми являются концепции:(https://blog.skillfactory.ru/glossary/kafka-apache/)
		продюсер (producer) — приложение или процесс, генерирующий и посылающий данные (публикующий сообщение);
		потребитель (consumer) — приложение или процесс, который принимает сгенерированное продюсером сообщение;
		сообщение — пакет данных, необходимый для совершения какой-либо операции (например, авторизации, оформления покупки или подписки);
		брокер — узел (диспетчер) передачи сообщения от процесса-продюсера приложению-потребителю;
		топик (тема) — виртуальное хранилище сообщений (журнал записей) одинакового или похожего содержания, из которого приложение-потребитель извлекает необходимую ему информацию.
		
	Консьюмеры получают данные с сервера, используя две разные модели запросов: pull или push.(https://habr.com/ru/companies/slurm/articles/550934/)
		pull-модель — консьюмеры сами отправляют запрос раз в n секунд на сервер для получения новой порции сообщений. 
			При таком подходе клиенты могут эффективно контролировать собственную нагрузку. Кроме того, pull-модель позволяет группировать сообщения в батчи, таким образом достигая лучшей пропускной способности. 
			К минусам модели можно отнести потенциальную разбалансированность нагрузки между разными консьюмерами, а также более высокую задержку обработки данных.
		push-модель — сервер делает запрос к клиенту, посылая ему новую порцию данных. По такой модели, например, работает RabbitMQ. 
			Она снижает задержку обработки сообщений и позволяет эффективно балансировать распределение сообщений по консьюмерам. 
			Но для предотвращения перегрузки консьюмеров в случае с RabbitMQ клиентам приходится использовать функционал QS, выставляя лимиты.
			
	Docs: https://habr.com/ru/companies/slurm/articles/550934/,
		https://blog.skillfactory.ru/glossary/kafka-apache/,
		https://medium.com/@kirill.sereda/kafka-%D0%B4%D0%BB%D1%8F-%D1%81%D0%B0%D0%BC%D1%8B%D1%85-%D0%BC%D0%B0%D0%BB%D0%B5%D0%BD%D1%8C%D0%BA%D0%B8%D1%85-f42864cb1bfb,
		https://cloud.google.com/learn/what-is-apache-kafka

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Salesforce(https://www.onlineinterviewquestions.com/salesforce-interview-questions/, https://www.sfdcpoint.com/salesforce/salesforce-developer-interview-questions/, https://intellipaat.com/blog/interview-question/salesforce-interview-questions/)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	Salesforce is Customer relationship management (CRM) software that is available to users as a software as a service (SaaS) application.
	In Salesforce an App is a group of tabs that work as a unit to provide functionality. 
		Users can switch between multiple apps using the Force.com app drop-down menu at the top-right corner of every page.
	Below are top benefits of using Salesforce CRM
		It Ensures faster and better sales opportunity
		Reducing cost and improving customer satisfaction level
		Deploying an analytical approach to customer acquisition
		Automation of repetitive tasks.
		
	There are two important relationships in Salesforce. They are
		Lookup relationship
		Master detail relationship
			Master-Detail Relationship is the Parent child relationship. In which Master represents Parent and detail represents Child.
			In this relationship,if parent is deleted then Child also gets deleted.
	There are three types of object realations are available in Salesforce , they are
		One to many.
		Many to many.
		Master detail
	
	There are two different types of Salesforce objects. 
		These are standard objects and custom objects. The standard objects are those provided by Salesforce such as accounts, contacts, leads, opportunities, cases, reports and dashboards. 
		The custom objects are created by the user.
	
	Custom Objects are nothing but database tables and are the objects created by you for the storage of information on company or industry. 
		While building a custom object, the Salesforce platform automatically builds things such as page layouts, etc for user interfaces.
	
	Custom tabs display custom object data or web content data embedded within your application. Custom Object tabs look similar to standard tabs.
	
	Standard Fields are predefined fields in Salesforce, cannot be deleted, can be removed from a page layout, and can have limited customizations: changing labels, adding help text, 
		adding/editing values in picklists, adding/editing lookup filters, setting field history tracking, changing the format of auto-number fields.
	
	Field-Level Security defines user ability to view and edit fields in Salesforce, helps enforce data security, helps ensure that users view only relevant data. 
		This affects list views, search results, and reports.
	
	Before and after changes are made to a record, the custom actions performed in Salesforce are called Triggers. 
		These actions include insertions, updates or deletions. Triggers can be used to invoke Apex and a trigger is nothing but an Apex code that is executed before 
			and after the following set of actions.
		
	Trigger takes a programmatic approach but the workflow does not require coding. When you want to take some actions say for an email, outbound message, task or field update, 
		then a point in click workflow is required.
	
	Force.com is a Platform-as-a-Service (PaaS) but Salesforce.com is a Software-as-a-Service (SaaS).
		Moreover, Salesforce.com comes as an out-of-the-box solution and comes along with three core products – Sales Cloud, Marketing Cloud, and Service Cloud. 
		It is possible to buy Sales Cloud and access leads, opportunities and reports.
	SOQL (Salesforce Object Query Language) only allows you to query, a single object at a time but SOSL (Salesforce Object Search Language) helps you in searching texts, 
		emails and phone fields with various objects. 
		SOQL can be used in triggers and classes but SOSL only used in classes.
	Profile is mandatory for all Salesforce users. No user can work in a Salesforce.org without a profile. It is for controlling the access to records in Salesforce.org for a specific user.
		On the other hand, a role controls the level of access the users have for the data. People with a role have the view, edit or report rights for all data owned/shared by lower-level users.
		
	Apex is a strongly typed, object-oriented programming language that allows developers to extend the Salesforce platform by writing their own business logic into the platform. 
		Apex looks similar to Java and can be launched through a variety of user-initiated events such as record updates, button clicks, triggers on objects, or external web service requests.
	Batch class in salesforce is used to run large jobs (think thousands or millions of records!) that would exceed normal processing limits. Using Batch Apex, 
		you can process records asynchronously in batches 
		(hence the name, “Batch Apex”) to stay within platform limits. If you have a lot of records to process, for example, data cleansing or archiving, Batch Apex is probably your best solution.
	An Apex transaction represents a set of operations that are executed as a single unit. These operations include DML operations that are responsible for querying records.
		All the DML operations in a transaction either get completed successfully or get rolled back completely if an error occurs even in saving a single record.
	There are three main types of collections…
		Lists – A list is an ordered collection of elements that are distinguished by their indices. List elements can be of any data type—primitive types, collections, sObjects,
			user-defined types, and built-in Apex types.
		Sets – A set is an unordered collection of elements that do not contain any duplicates. Set elements can be of any data type—primitive types, collections, sObjects, 
			user-defined types, and built-in Apex types.
		Maps – A map is a collection of key-value pairs where each unique key maps to a single value. Keys and values can be any data type—primitive types, collections, sObjects, 
			user-defined types, and built-in Apex types.
	
	Salesforce has a variety of API’s that let you interact with the system in different ways..
		REST – The REST API lets you integrate with Force.com applications using simple HTTP methods in either XML or JSON formats,
			making this an idea API for developing mobile applications or external clients.
		Bulk – The Bulk API provides a programmatic access that lets you quickly load data into your Salesforce organisation.
		Streaming – The Streaming API can be used to receive notifications for changes to Salesforce data that match a SOQL query you define. 
			Streaming API is useful when you want notifications to be pushed from the server to the client based on criteria that you define.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Big data (https://intellect.ml/big-data-chast-1-printsipy-raboty-s-bolshimi-dannymi-paradigma-mapreduce-6815)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	Большие данные (англ. big data) — серия подходов, инструментов и методов обработки структурированных и неструктурированных данных огромных объёмов
	 и значительного многообразия для получения воспринимаемых человеком результатов,
	 эффективных в условиях непрерывного прироста, распределения по многочисленным узлам вычислительной сети.
	 
		-- Принципы работы с большими данными
			1. Горизонтальная масштабируемость. Cистема, которая подразумевает обработку больших данных, должна быть расширяемой.
			2. Отказоустойчивость. Принцип горизонтальной масштабируемости подразумевает, что машин в кластере может быть много.
			3. Локальность данных. По возможности обрабатываем данные на той же машине, на которой их храним. 

	Кластер — группа компьютеров, объединённых высокоскоростными каналами связи, представляющая с точки зрения пользователя единый аппаратный ресурс. 
	Кластер - слабо связанная совокупность нескольких вычислительных систем, работающих совместно для выполнения общих приложений, и представляющихся пользователю единой системой.
	
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Hadoop Distributed File System (HDFS) (https://uk.wikipedia.org/wiki/Hadoop_Distributed_Filesystem)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
			
	Hadoop Distributed File System (HDFS) - це розподілена файлова система, яка забезпечує високошвидкісний доступ до даних і є одним з ключових компонентів платформи Hadoop. 
	HDFS - це файлова система на основі Java, яка забезпечує масштабуємість і надійне зберігання даних, призначена для розбиття великих кластерів на стандартних серверах. 
	HDFS, MapReduce та YARN утворюють ядро Apache Hadoop.
	
	HDFS є ієрархічною файловою системою. Таким чином, в HDFS є підтримка вкладення каталогів. У каталозі може розташовуватися нуль або більше файлів, а також будь-яка кількість підкаталогів.
	HDFS складається з наступних обов'язкових компонентів:
		Вузол імен (NameNode) - програмний код, що виконується, в загальному випадку, на виділеній машині примірника HDFS і відповідає за файлові операції (роботу з метаданими);
			- управління простором імен файлової системи
			- управління доступом з боку зовнішніх клієнтів
			- відповідність між файлами і реплицироваться на вузлах даних блоками
		Вузол даних (DataNode) - програмний код, як правило, виконується виділеної машині примірника HDFS і відповідає за операції рівня файлу (робота з блоками даних).
			- періодичну відправку повідомлення про стан (heartbeat-повідомлення)
			- обробку запитів на читання і запис, що надходять від клієнтів файлової системи HDFS
	
	Основні концепції, закладені при проектуванні HDFS
		Обсяг даних. HDFS не повинна мати досяжних в осяжному майбутньому обмежень на обсяг збережених даних.
		Відмовостійкість. HDFS розцінює вихід з ладу вузла даних як норму, а не як виняток (дійсно, ймовірність виходу хоча б одного вузла з тисячі навіть на надійному фізичному обладнанні істотна).
		Автодіагностика. Діагностика справності вузлів в Hadoop-кластері не повинна вимагати додаткового адміністрування.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
MapReduce (https://intellect.ml/big-data-chast-1-printsipy-raboty-s-bolshimi-dannymi-paradigma-mapreduce-6815)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	MapReduce – это модель распределенной обработки данных, предложенная компанией Google для обработки больших объёмов данных на компьютерных кластерах.
	MapReduce предполагает, что данные организованы в виде некоторых записей. Обработка данных происходит в 3 стадии: 
		
		1. Стадия Map. На этой стадии данные предобрабатываются при помощи функции map(), которую определяет пользователь.
			Функция map() примененная к одной входной записи и выдаёт множество пар ключ-значение.

		2. Стадия Shuffle. Проходит незаметно для пользователя. В этой стадии вывод функции map «разбирается по корзинам» – каждая корзина соответствует одному ключу вывода стадии map. 
			В дальнейшем эти корзины послужат входом для reduce. 
		
		3. Стадия Reduce. Каждая «корзина» со значениями, сформированная на стадии shuffle, попадает на вход функции reduce(). 
			Функция reduce задаётся пользователем и вычисляет финальный результат для отдельной «корзины». 
			Множество всех значений, возвращённых функцией reduce(), является финальным результатом MapReduce-задачи. 
			
		Работа MapReduce состоит из двух шагов: Map и Reduce, названных так по аналогии с одноименными функциями высшего порядка, map и reduce.
			На Map-шаге происходит предварительная обработка входных данных. Для этого один из компьютеров (называемый главным узлом — master node) получает входные данные задачи,
				разделяет их на части и передает другим компьютерам (рабочим узлам — worker node) для предварительной обработки.
			На Reduce-шаге происходит свёртка предварительно обработанных данных.
				Главный узел получает ответы от рабочих узлов и на их основе формирует результат — решение задачи, которая изначально формулировалась.
		
	Несколько дополнительных фактов про MapReduce: 
		1) Все запуски функции map работают независимо и могут работать параллельно, в том числе на разных машинах кластера. 
		2) Все запуски функции reduce работают независимо и могут работать параллельно, в том числе на разных машинах кластера. 
		3) Shuffle внутри себя представляет параллельную сортировку, поэтому также может работать на разных машинах кластера. 
			Пункты 1-3 позволяют выполнить принцип горизонтальной масштабируемости. 
		4) Функция map, как правило, применяется на той же машине, на которой хранятся данные – это позволяет снизить передачу данных по сети (принцип локальности данных). 
		5) MapReduce – это всегда полное сканирование данных, никаких индексов нет. Это означает, что MapReduce плохо применим, когда ответ требуется очень быстро. 

		
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
YARN (https://www.ibm.com/developerworks/ru/library/bd-yarn-intro/)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
		
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Apache Hadoop (https://uk.wikipedia.org/wiki/Apache_Hadoop)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
		
	Apache Hadoop — вільна програмна платформа і каркас для організації розподіленого зберігання і обробки наборів великих даних з використанням моделі програмування MapReduce,
		при якій завдання ділиться на багато дрібніших відособлених фрагментів, кожен з яких може бути запущений на окремому вузлі кластера, що складається з серійних комп'ютерів.
		Всі модулі в Hadoop спроектовані з врахуванням припущення, що злам апаратного забезпечення трапляється часто і це повинно автоматично враховуватись фреймворком.
	Основний фреймворк Apache Hadoop складається з наступних модулів:
		Hadoop Common — містить бібліотеки та утиліти потрібні іншим модулям Hadoop;
		Hadoop Distributed File System (HDFS) — розподілена файлова система, яка зберігає дані на звичайних машинах, надаючи дуже високу загальну пропускну здатність на кластері загалом;
		Hadoop YARN — платформа що відповідає за керування обчислювальними ресурсами в кластерах і їх використання для користувацьких завдань.
		Hadoop MapReduce — реалізація моделі програмування MapReduce для обробки великих об'ємів даних.
		
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Apache Spark (https://ru.wikipedia.org/wiki/Apache_Spark)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	Apache Spark (от англ. spark — искра, вспышка) — программный каркас с открытым исходным кодом для реализации распределённой обработки неструктурированных и слабоструктурированных данных,
		входящий в экосистему проектов Hadoop. В отличие от классического обработчика из ядра Hadoop, реализующего двухуровневую концепцию MapReduce с дисковым хранилищем,
		использует специализированные примитивы для рекурентной обработки в оперативной памяти, благодаря чему позволяет получать значительный выигрыш в скорости работы для некоторых классов задач,
		в частности, возможность многократного доступа к загруженным в память пользовательским данным делает библиотеку привлекательной для алгоритмов машинного обучения.
		
	(https://proglib.io/p/spark-overview)	
	Spark состоит из ряда библиотек, которые созданы для решения задач Data Science. 
		Spark включает библиотеки для SQL (SparkSQL), машинного обучения (MLlib), обработки потоковых данных (Spark Streaming и Structured Streaming) и обработки графов (GraphX).
	Каждое Spark-приложение состоит из управляющего процесса – драйвера (Driver) – и набора распределённых рабочих процессов – исполнителей (Executors).
	Driver запускает метод main() нашего приложения. Здесь создаётся SparkContext.
	Исполнитель (Executor) – распределённый процесс, который отвечает за выполнение задач. У каждого приложения Spark собственный набор исполнителей. 
		Они работают в течение жизненного цикла отдельного приложения Spark.	
	Когда отправляем задание в Spark для обработки, многое остаётся за кулисами.
		Наше автономное приложение запускается и инициализирует SparkContext. Только при наличии SparkContext приложение называется драйвером.
		Наша программа-драйвер (Driver program) запрашивает у менеджера кластеров (Cluster Manager) ресурсы для запуска исполнителей.
		Менеджер кластеров запускает исполнителей.
		Наш драйвер запускает собственно код Spark.
		Исполнители запускают задания и отправляют результаты драйверу.
		SparkContext останавливается, а исполнители закрываются и возвращают ресурсы обратно в кластер.
	RDD – строительные блоки Spark: всё состоит из них. Даже высокоуровневые Spark API (DataFrames, Datasets) состоят из RDD под капотом. 
		Resilient – Устойчивый: поскольку Spark работает на кластере компьютеров, потеря данных из-за аппаратного сбоя представляет собой серьёзную проблему, поэтому RDD отказоустойчивые и восстанавливаются в случае сбоя.
		Distributed – Распределённый: один RDD хранится на нескольких узлах кластера, которые не принадлежат одному источнику (и одной точке отказа). Таким образом, кластер оперирует RDD параллельно.
		Dataset – Набор данных: коллекция значений – вы наверняка уже знали это.
	RDD Immutable, это означает, что после создания эти наборы никак не изменяются, а только трансформируются (transformed).
	Spark определяет набор API для работы с RDD, которые разбиты на две большие группы: Трансформации и Действия .
		map-функция weatherData.map() – это трансформация, которая передаёт каждый элемент RDD в функцию.
		Reduce – это действие RDD, которое объединяет все элементы RDD с использованием некоторой функции и возвращает конечный результат в программу-драйвер.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Hbase (https://intellect.ml/big-data-ot-chast-4-hbase-6819)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	Hbase — это распределенная, колоночно-ориентированная, мультиверсионная база типа «ключ-значение». 
	Данные организованы в таблицы, проиндексированные первичным ключом, который в Hbase называется RowKey.  
	Для каждого RowKey ключа может храниться неограниченны набор атрибутов (или колонок).  
	Колонки организованны в группы колонок, называемые Column Family. Как правило в одну Column Family объединяют колонки, для которых одинаковы паттерн использования и хранения.  
	Для каждого аттрибута может храниться несколько различных версий. Разные версии имеют разный timestamp. Записи физически хранятся в отсортированном по RowKey порядке. 
	При этом данные соответствующие разным Column Family хранятся отдельно, что позволяет при необходимости читать данные только из нужного семейства колонок.  
	При удалении определённого атрибута физически он сразу не удаляется, а лишь маркируется специальным флажком tombstone. Физическое удаление данных произойдет позже, при выполнении операции Major Compaction. 
	Атрибуты, принадлежащие одной группе колонок и соответствующие одному ключу физически хранятся как отсортированный список. 
	Любой атрибут может отсутствовать или присутствовать для каждого ключа, при этом если атрибут отсутствует — это не вызывает накладных расходов на хранение пустых значений. 
	Список и названия групп колонок фиксирован и имеет четкую схему. На уровне группы колонок задаются такие параметры как time to live (TTL) и максимальное количество хранимых версий. 
	Если разница между timestamp для определенно версии и текущим временем больше TTL — запись помечается к удалению. 
	Если количество версий для определённого атрибута превысило максимальное количество версий — запись также помечается к удалению.

	Поддерживаемые операции Список поддерживаемых операций в hbase весьма прост. Поддерживаются 4 основные операции: 
		— Put: добавить новую запись в hbase. Timestamp этой записи может быть задан руками, в противном случае он будет установлен автоматически как текущее время. 
		— Get: получить данные по определенному RowKey. Можно указать Column Family, из которой будем брать данные и количество версий которые хотим прочитать.  
		— Scan: читать записи по очереди. Можно указать запись с которой начинаем читать, запись до которой читать, количество записей которые необходимо считать, 
			Column Family из которой будет производиться чтение и максимальное количество версий для каждой записи.  
		— Delete: пометить определенную версию к удалению. Физического удаления при этом не произойдет, оно будет отложено до следующего Major Compaction
		
	Hbase для своей работы использует два основных процесса: 
		1. Region Server — обслуживает один или несколько регионов. Регион — это диапазон записей соответствующих определенному диапазону подряд идущих RowKey.
			Persistent Storage — основное хранилище данных в Hbase. Данные физически хранятся на HDFS, в специальном формате HFile. 
				Данные в HFile хранятся в отсортированном по RowKey порядке. Одной паре (регион, column family) соответствует как минимум один HFIle.
			MemStore — буфер на запись. Так как данные хранятся в HFile d отсортированном порядке — обновлять HFile на каждую запись довольно дорого. 
				Вместо этого данные при записи попадают в специальную область памяти MemStore, где накапливаются некоторое время. 
				При наполнении MemStore до некоторого критического значения данные записываются в новый HFile.
			BlockCache — кэш на чтение. Позволяет существенно экономить время на данных которые читаются часто.
			Write Ahead Log(WAL). Так как данные при записи попадают в memstore, существует некоторый риск потери данных из-за сбоя. 
				Для того чтобы этого не произошло все операции перед собственно осуществление манипуляций попадают в специальный лог-файл. 
				Это позволяет восстановить данные после любого сбоя.
		2. Master Server — главный сервер в кластере hbase. 
		Master управляет распределением регионов по Region Server’ам, ведет реестр регионов, управляет запусками регулярных задач и делает другую полезную работу. 
			Minor Compaction. Запускается автоматически, выполняется в фоновом режиме. Имеет низкий приоритет по сравнению с другими операциями Hbase. 
			Major Compaction. Запускается руками или по наступлению срабатыванию определенных триггеров(например по таймеру). 
				Имеет высокий приоритет и может существенно замедлить работу кластера. Major Compaction’ы лучше делать во время когда нагрузка на кластер небольшая. 
				Во время Major Compaction также происходит физическое удаление данных, ране помеченных меткой tombstone. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

-- ------------------------------------------------ Solr
Solr (произносится «солар»[1]) — платформа полнотекстового поиска с открытым исходным кодом, основанная на проекте Apache Lucene. 
Её основные возможности: полнотекстовый поиск, подсветка результатов, фасетный поиск, динамическая кластеризация, интеграция с базами данных, обработка документов со сложным форматом (например, Word, PDF). 
Так как в Solr есть возможность распределенного поиска и репликации, Solr хорошо масштабируем[2].


Первые версии программ полнотекстового поиска предполагали сканирование всего содержимого всех документов в поиске заданного слова или фразы. 
При использовании такой технологии поиск занимал очень много времени (в зависимости от размера базы), а в интернете был бы невыполним.
 Современные алгоритмы заранее формируют для поиска так называемый полнотекстовый индекс — словарь, в котором перечислены все слова и указано, в каких местах они встречаются. 
При наличии такого индекса достаточно осуществить поиск нужных слов в нём и тогда сразу же будет получен список документов, в которых они встречаются.




-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
-- ------------------------------------------------ ANTLR(https://github.com/antlr/antlr4/blob/master/doc/index.md, https://ru.wikipedia.org/wiki/ANTLR)
		ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files.
			It's widely used to build languages, tools, and frameworks. From a grammar, ANTLR generates a parser that can build and walk parse trees.


		